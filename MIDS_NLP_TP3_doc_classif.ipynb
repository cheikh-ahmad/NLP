{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 3 : classification de documents\n",
    "\n",
    "(inspiré pour partie d'un TP par Antoine Simoulin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **classification de textes** est la tâche qui prend en entrée du texte, et prédit en sortie une ou plusieurs classes pour ce texte, l'important étant que l'ensemble des classes possibles est connu et fixé a priori.\n",
    "\n",
    "## Exemples de tâches relevant de la \"classification de texte\"\n",
    "\n",
    "De nombreuses applications de TAL correspondent à de la classification de textes. Par exemple :\n",
    "* L'\"**analyse de sentiments**\" (ou \"sentiment analysis\"): est un nom pompeux pour la tâche d'identification de  la polarité positive ou négative d'un texte, appliqué par exemple pour déterminer\n",
    "  * ce client est-il content ou pas?\n",
    "  * ce spectateur a-t-il aimé ce film ou pas?\n",
    "  * ...\n",
    "* La détection de Spam dans les Emails\n",
    "* Le suivi de tendances sur les réseaux sociaux\n",
    "* Recherche de réponse dans une FAQ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les données \n",
    "\n",
    "On utilise les données de classification de textes pour le français, telles qu'inclusent dans les données FLUE <a href=\"https://aclanthology.org/2020.lrec-1.302/\">(Le et al., 2020)</a>, cf. le repo https://github.com/getalp/Flaubert/tree/master/flue.\n",
    "\n",
    "Nous utilisons ici un extrait de la partie française d'un corpus d'\"analyse de sentiment\" multilingue, le corpus CLS <a href=\"https://aclanthology.org/P10-1114/\">(Prettenhofer and Stein, 2010)</a>.\n",
    "\n",
    "Les textes sont des revues par des utilisateurs, issues du site Amazon pour trois catégories de produits (livres, DVD et musique). Initialement, chaque exemple contenait une revue associée à une note allant de 1 à 5, mais les revues avec une note de 3 ont été écartées, et les notes ont été binarisées: 1/2 devient 0 (\"négatif\"), et 4/5 devient 1 (\"positif\"). \n",
    "\n",
    "Pour chaque catégorie de produit (livres, dvd, musique), les données cls-fr de FLUE contiennent 2000 revues de test, et 2000 revues d'apprentissage. Pour ce TP, nous avons en outre divisé les 2000 revues d'apprentissage en 1600 pour l'entraînement, et 400 pour la validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# ⚠️ Execute only if running in Colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  IN_COLAB = True\n",
    "else:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "  !pip install -q scikit-learn==0.23.2 matplotlib==3.3.2 pandas==1.1.3 nltk==3.5 spacy==2.3.2 \n",
    "  !python3 -m spacy download fr_core_news_md\n",
    "  # if running Colab, restart after libraries installation (Redémarrer l'environnement d'exécution)\n",
    "  # exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np  # python base math library\n",
    "import pandas as pd # data structure\n",
    "\n",
    "from collections import Counter\n",
    "#from pprint import pprint\n",
    "#from time import time\n",
    "import logging\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "# IPython automatically reload all changed code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Inline Figures with matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cls_file(file_path):\n",
    "    reviews, labels = [], []\n",
    "    with open(file_path,encoding='utf-8') as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            review, label = line.strip().split('\\t')\n",
    "            reviews.append(review.strip(\"\\\"\"))\n",
    "            labels.append(int(label))\n",
    "            line = f.readline()\n",
    "    return reviews, labels\n",
    "\n",
    "def load_cls_dataset(file_path, section='books'):\n",
    "    part2X, part2y = {}, {}\n",
    "    for part in ['train', 'valid']:  \n",
    "        part2X[part], part2y[part] = load_cls_file(os.path.join(file_path, section, part+'_0.tsv'))\n",
    "    return part2X, part2y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data_flue_cls_fr/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = {} # key1=cat, key2=part, val= list of reviews\n",
    "y = {} # key1=cat, key2=part, val= list of labels\n",
    "y_train = [] # les labels \"gold\" pour la partie train\n",
    "y_valid = [] # les labels \"gold\" pour la partie validation\n",
    "for genre in ['books', 'dvd', 'music']:\n",
    "    X[genre], y[genre] = load_cls_dataset(data_dir, genre)\n",
    "    y_train += y[genre]['train']\n",
    "    y_valid += y[genre]['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {} # key=cat, val = data frame\n",
    "for part in ['train', 'valid']:\n",
    "    df[part] = pd.DataFrame.from_dict(\n",
    "    {'review': X['books'][part] + X['dvd'][part] + X['music'][part],\n",
    "     'label': y['books'][part] + y['dvd'][part] + y['music'][part],\n",
    "     'genre': ['books' for _ in range(len(X['books'][part]))] \\\n",
    "             + ['dvd' for _ in range(len(X['dvd'][part]))] \\\n",
    "             + ['music' for _ in range(len(X['music'][part]))]})\n",
    "\n",
    "id2label = ['Négatif', 'Positif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Je voulais mettre 0 étoile mais c'est pas possible... Commençons par le positif (c'est rapide): ça parle de tout : muxle, endurance, souplesse, alimentation, échauffement mais ... mal, désespérement mal. L'auteur prétend avoir fait 15 ans de recherche avant de pondre son opus majus. Ben faudrait...</td>\n",
       "      <td>0</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Le récit de vie d'une femme américaine, avec ses contradictions, ses rêves de liberté, de justice (dans l'amérique de la guerre du Vietnam). Un récit qui n'est pas prêt de me quitter sur la complexité de la nature humaine dans ce qu'elle a de plus tortueux et de plus insaisissable. Un récit cons...</td>\n",
       "      <td>1</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quel chef d'oeuvre que cette 'autobiographie' ! C'est à croire que la vie de Michaël Crichton est aussi palpitante, voire plus par moments, que celle des personnages de ses romans... Que d'aventures, de découvertes, d'expérience ! On comprend mieux, après la lecture de cet ouvrage, d'où lui vien...</td>\n",
       "      <td>1</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Si vous cherchez un livre simple d'initiation à Scheme pour comprendre et modifier les Script-Fu de Gimp, ne faites pas comme moi, choisissez-en un autre. Si vous voulez étudier ce langage d'un point de vue théorique, sans allumer votre ordinateur, en entrant dans le détail des algorithmes, sans...</td>\n",
       "      <td>1</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ce livre est très intéressant à lire à plus d'un titre. Certes JF Revel voit juste dans son analyse au sujet de la pensée ambiante vis-à-vis des USA. Cependant il manque totalement de rigueur scientifique (beaucoup d'assertions sans références bibliographiques). Cet ouvrage bien qu'éclairant ne ...</td>\n",
       "      <td>0</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>J'avais déjà été déçue par \"\"Dossier Benton\"\". Mais là, que dire de \"\"L'Ile des Chiens\"\" ??? Quel ennui ! Les personnages sont burlesques, et l'intrigue initéressante ! Je suis pourtant une inconditionnelle de celle que je considérais comme LA reine du thriller. On ne ressuscitera pas Benton, ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Michel Hoàng trace un portrait captivant du plus grand conquérant de l'histoire. Il dépeint les mœurs de ces nomades de la steppe qui engendreront Tèmudjin, le fils de petit chef qui s'élèvera au range de Khan océanique. L'ascension lente et patiente faite à coup d'alliances et « desalliance » d...</td>\n",
       "      <td>1</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Que lui avons-nous fait? Pourquoi la belle Amélie se croit-elle obligée de nous décevoir chaque année? Si le but est de nous prouver que le génie ne s'utilise pas comme une clé de 12, la démonstration en a été faite depuis longtemps.</td>\n",
       "      <td>0</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Il y a deux manières d'envisager ce livre. Sur le plan de la méthode, Encel expose le Golan d'un point de vue géopolitique, ce qui peut être pertinent... jusqu'à un certain point. En effet, au-delà de certaines imprécisions, voire confusions dans la réalité historique du Golan, l'auteur en vient...</td>\n",
       "      <td>0</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>On s'attend avec ce livre à découvrir un pays, une époque, et surtout lire un roman d'espionnage, quelque chose de palpitant, qui s'emballe, un livre que l'on ne peut plus quitter quand on l'a ouvert. Malheureusement, le scénario s'essouffle au fil des pages, pour devenir carrément asthmatique à...</td>\n",
       "      <td>0</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                        review  \\\n",
       "0  Je voulais mettre 0 étoile mais c'est pas possible... Commençons par le positif (c'est rapide): ça parle de tout : muxle, endurance, souplesse, alimentation, échauffement mais ... mal, désespérement mal. L'auteur prétend avoir fait 15 ans de recherche avant de pondre son opus majus. Ben faudrait...   \n",
       "1  Le récit de vie d'une femme américaine, avec ses contradictions, ses rêves de liberté, de justice (dans l'amérique de la guerre du Vietnam). Un récit qui n'est pas prêt de me quitter sur la complexité de la nature humaine dans ce qu'elle a de plus tortueux et de plus insaisissable. Un récit cons...   \n",
       "2  Quel chef d'oeuvre que cette 'autobiographie' ! C'est à croire que la vie de Michaël Crichton est aussi palpitante, voire plus par moments, que celle des personnages de ses romans... Que d'aventures, de découvertes, d'expérience ! On comprend mieux, après la lecture de cet ouvrage, d'où lui vien...   \n",
       "3  Si vous cherchez un livre simple d'initiation à Scheme pour comprendre et modifier les Script-Fu de Gimp, ne faites pas comme moi, choisissez-en un autre. Si vous voulez étudier ce langage d'un point de vue théorique, sans allumer votre ordinateur, en entrant dans le détail des algorithmes, sans...   \n",
       "4  Ce livre est très intéressant à lire à plus d'un titre. Certes JF Revel voit juste dans son analyse au sujet de la pensée ambiante vis-à-vis des USA. Cependant il manque totalement de rigueur scientifique (beaucoup d'assertions sans références bibliographiques). Cet ouvrage bien qu'éclairant ne ...   \n",
       "5  J'avais déjà été déçue par \"\"Dossier Benton\"\". Mais là, que dire de \"\"L'Ile des Chiens\"\" ??? Quel ennui ! Les personnages sont burlesques, et l'intrigue initéressante ! Je suis pourtant une inconditionnelle de celle que je considérais comme LA reine du thriller. On ne ressuscitera pas Benton, ma...   \n",
       "6  Michel Hoàng trace un portrait captivant du plus grand conquérant de l'histoire. Il dépeint les mœurs de ces nomades de la steppe qui engendreront Tèmudjin, le fils de petit chef qui s'élèvera au range de Khan océanique. L'ascension lente et patiente faite à coup d'alliances et « desalliance » d...   \n",
       "7                                                                    Que lui avons-nous fait? Pourquoi la belle Amélie se croit-elle obligée de nous décevoir chaque année? Si le but est de nous prouver que le génie ne s'utilise pas comme une clé de 12, la démonstration en a été faite depuis longtemps.   \n",
       "8  Il y a deux manières d'envisager ce livre. Sur le plan de la méthode, Encel expose le Golan d'un point de vue géopolitique, ce qui peut être pertinent... jusqu'à un certain point. En effet, au-delà de certaines imprécisions, voire confusions dans la réalité historique du Golan, l'auteur en vient...   \n",
       "9  On s'attend avec ce livre à découvrir un pays, une époque, et surtout lire un roman d'espionnage, quelque chose de palpitant, qui s'emballe, un livre que l'on ne peut plus quitter quand on l'a ouvert. Malheureusement, le scénario s'essouffle au fil des pages, pour devenir carrément asthmatique à...   \n",
       "\n",
       "   label  genre  \n",
       "0      0  books  \n",
       "1      1  books  \n",
       "2      1  books  \n",
       "3      1  books  \n",
       "4      0  books  \n",
       "5      0  books  \n",
       "6      1  books  \n",
       "7      0  books  \n",
       "8      0  books  \n",
       "9      0  books  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 300\n",
    "df['train'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de commencer à dérouler un cas d'usage, vous devez toujours **analyser les données**. Vérifiez les valeurs manquantes ou abérrantes, la distribution des variables, l'équilibre des classes, sélectionnez ou écartez les variables en fonction de leur pertinence ou selon des critères éthiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO1: Comparer la distribution des labels positifs / négatifs, et des genres (music, dvd, books) entre le corpus train et le corpus de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'label', 'genre'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['train'].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-traitements des textes: tokenisation, lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO2 : normalisation du vocabulaire\n",
    "\n",
    "- tokenisez et lemmatisez les revues en utilisant Spacy (cf. TP2)\n",
    "  * NB: dans le pipeline spacy, ci-dessous, on ne conserve que les modules nécessaires à la lemmatisation\n",
    "- vous **stockerez** ces informations dans la DataFrame, pour chaque revue, les champs \"tokens\" et \"lemmas\"\n",
    "\n",
    "- **comparez** les tailles de vocabulaire, pour les revues du \"train\", pour les 2 types de tokens\n",
    "  -- les tokens obtenus après tokenization spacy\n",
    "  -- les lemmes obtenus après lemmatisation spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current pipeline:\n",
      "  tok2vec\n",
      "  morphologizer\n",
      "  lemmatizer\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "try:\n",
    "    #rem: enable does not seem to work, don't know why\n",
    "    nlp = spacy.load(\"fr_core_news_md\") #, enable=[\"tok2vec\", \"morphologizer\", \"lemmatizer\"])\n",
    "except OSError:\n",
    "    !python -m spacy download fr_core_news_md\n",
    "    nlp = spacy.load(\"fr_core_news_md\") #, enable=[\"tok2vec\", \"morphologizer\", \"lemmatizer\"])\n",
    "\n",
    "# we won't need ner, parser, attribute_ruler\n",
    "# NB: tok2vec and morphologizer seem to be necessary for lemmatization\n",
    "nlp.remove_pipe(\"ner\")\n",
    "nlp.remove_pipe(\"parser\")\n",
    "nlp.remove_pipe(\"attribute_ruler\")\n",
    "print('Current pipeline:\\n  '+'\\n  '.join(nlp.pipe_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7fa6417a6f40>)\n",
      "('morphologizer', <spacy.pipeline.morphologizer.Morphologizer object at 0x7fa6417a6e20>)\n",
      "('senter', <spacy.pipeline.senter.SentenceRecognizer object at 0x7fa6417a64c0>)\n",
      "('lemmatizer', <spacy.lang.fr.lemmatizer.FrenchLemmatizer object at 0x7fa62f019780>)\n",
      "rule\n"
     ]
    }
   ],
   "source": [
    "# remarque: le type de lemmatiseur Spacy, pour le modèle fr_core_news_md est \"rule\" = \"par règles\"\n",
    "for c in nlp.components:\n",
    "    print(c)\n",
    "    if c[0] == 'lemmatizer':\n",
    "        print(c[1].mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"Je voulais mettre 0 étoile mais c'est pas possible...\n",
    " Commençons par le positif (c'est rapide): ça parle de tout : \n",
    "muxle, endurance, souplesse, alimentation, échauffement mais ... mal, désespérement mal.\"\"\"\n",
    "\n",
    "# testez la tokenization / lemmatisation sur le texte supra\n",
    "# puis appliquez à l'ensemble des revues\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# application aux revues de la lemmatisation Spacy et la racinisation NLTK\n",
    "# NB: il est important de n'appeler qu'une seule fois le traitement spacy sur un texte\n",
    "#     cf. coûteux en temps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Je voulais mettre 0 étoile mais c'est pas possible... Commençons par le positif (c'est rapide): ça parle de tout : muxle, endurance, souplesse, alimentation, échauffement mais ... mal, désespérement mal. L'auteur prétend avoir fait 15 ans de recherche avant de pondre son opus majus. Ben faudrait...</td>\n",
       "      <td>0</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Le récit de vie d'une femme américaine, avec ses contradictions, ses rêves de liberté, de justice (dans l'amérique de la guerre du Vietnam). Un récit qui n'est pas prêt de me quitter sur la complexité de la nature humaine dans ce qu'elle a de plus tortueux et de plus insaisissable. Un récit cons...</td>\n",
       "      <td>1</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quel chef d'oeuvre que cette 'autobiographie' ! C'est à croire que la vie de Michaël Crichton est aussi palpitante, voire plus par moments, que celle des personnages de ses romans... Que d'aventures, de découvertes, d'expérience ! On comprend mieux, après la lecture de cet ouvrage, d'où lui vien...</td>\n",
       "      <td>1</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Si vous cherchez un livre simple d'initiation à Scheme pour comprendre et modifier les Script-Fu de Gimp, ne faites pas comme moi, choisissez-en un autre. Si vous voulez étudier ce langage d'un point de vue théorique, sans allumer votre ordinateur, en entrant dans le détail des algorithmes, sans...</td>\n",
       "      <td>1</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ce livre est très intéressant à lire à plus d'un titre. Certes JF Revel voit juste dans son analyse au sujet de la pensée ambiante vis-à-vis des USA. Cependant il manque totalement de rigueur scientifique (beaucoup d'assertions sans références bibliographiques). Cet ouvrage bien qu'éclairant ne ...</td>\n",
       "      <td>0</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                        review  \\\n",
       "0  Je voulais mettre 0 étoile mais c'est pas possible... Commençons par le positif (c'est rapide): ça parle de tout : muxle, endurance, souplesse, alimentation, échauffement mais ... mal, désespérement mal. L'auteur prétend avoir fait 15 ans de recherche avant de pondre son opus majus. Ben faudrait...   \n",
       "1  Le récit de vie d'une femme américaine, avec ses contradictions, ses rêves de liberté, de justice (dans l'amérique de la guerre du Vietnam). Un récit qui n'est pas prêt de me quitter sur la complexité de la nature humaine dans ce qu'elle a de plus tortueux et de plus insaisissable. Un récit cons...   \n",
       "2  Quel chef d'oeuvre que cette 'autobiographie' ! C'est à croire que la vie de Michaël Crichton est aussi palpitante, voire plus par moments, que celle des personnages de ses romans... Que d'aventures, de découvertes, d'expérience ! On comprend mieux, après la lecture de cet ouvrage, d'où lui vien...   \n",
       "3  Si vous cherchez un livre simple d'initiation à Scheme pour comprendre et modifier les Script-Fu de Gimp, ne faites pas comme moi, choisissez-en un autre. Si vous voulez étudier ce langage d'un point de vue théorique, sans allumer votre ordinateur, en entrant dans le détail des algorithmes, sans...   \n",
       "4  Ce livre est très intéressant à lire à plus d'un titre. Certes JF Revel voit juste dans son analyse au sujet de la pensée ambiante vis-à-vis des USA. Cependant il manque totalement de rigueur scientifique (beaucoup d'assertions sans références bibliographiques). Cet ouvrage bien qu'éclairant ne ...   \n",
       "\n",
       "   label  genre  \n",
       "0      0  books  \n",
       "1      1  books  \n",
       "2      1  books  \n",
       "3      1  books  \n",
       "4      0  books  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['train'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparaison des tailles de vocabulaires obtenues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentation vectorielle de chaque revue: encodage \"bag-of-word\" (BOW) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-RwI4kdjnrw"
   },
   "source": [
    "En classification automatique, les \"objets\" à classer doivent être représentés sous forme de vecteur, qui constitue l'entrée du classifieur.\n",
    "En classif de documents, ces objets sont des documents.\n",
    "Et on a vu en cours la représentation vectorielle la plus basique : Bag-of-Words (BOW)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w2MX8tqsjnrw"
   },
   "source": [
    "**Rappel: Bag of Words** : On attribue un indice à chacun des mots du vocabulaire défini par le corpus d'entrainement. On peut ensuite représenter chaque document par un vecteur X indiquant, à la composante i le nombre d'occurrences du i-ème mot du vocabulaire. La taille du vocabulaire est généralement comprise entre 30.000 et 100.000 mots. Un vecteur BOW a bcp de valeurs nulles, cf. un document ne couvre qu'une toute petite partie du vocabulaire: un vecteur BOW est **creux** (\"sparse\" en anglais) et de grande taille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On considère ce corpus \"jouet\", constitué de 3 documents déjà tokenisés,\n",
    "# i.e. chaque document est une liste de formes.\n",
    "\n",
    "train_corpus = [\n",
    "    [\"je\", \"n'\", \"aime\", \"pas\", \"ce\", \"livre\"],\n",
    "    [\"un\", \"livre\", \"très\", \"complet\", \",\", \"un\", \"livre\", \"magique\"],\n",
    "    [\"pas\", \"un\", \"livre\", \"exceptionnel\", \",\", \"ni\", \"un\", \"livre\", \"très\", \"complet\"]]\n",
    "\n",
    "test_corpus = [\n",
    "    [\"un\", \"nouveau\", \"livre\", \"super\"],\n",
    "    [\"et\", \"un\", \"autre\", \"livre\", \"magique\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO3: représentation BOW d'après un vocabulaire\n",
    "\n",
    "La première chose à faire est d'associer chaque mot du vocabulaire à un identifiant entier. On part en général de 0. L'ordre n'a pas d'importance.\n",
    "\n",
    "En général, le vocabulaire est défini comme tous les mots rencontrés dans un certain ensemble de documents (en général, l'ensemble d'apprentissage, train)).\n",
    "\n",
    "* écrire une fonction **get_vocab** qui parcourt un ens. de documents tokenises (une liste de liste de tokens) et en ressort\n",
    "  * un dictionnaire **w2id** : clé = mot, valeur = id du mot\n",
    "    * pour aller des mots vers leur id\n",
    "  * une liste **id2w**, où au rang i dans la liste, se trouve le mot d'identifiant i\n",
    "    * pour récupérer d'un id vers le mot auquel il correspond\n",
    "* écrire une fonction bow qui rend le vecteur bow d'un document (sous forme de liste, où le rang correspond à l'id d'un mot du vocabulaire)\n",
    "* appliquez vos fonctions pour afficher les vecteurs BOW obtenus pour le mini corpus train_corpus et test_corpus\n",
    "  * NB: le vocabulaire utilisé est celui du train_corpus\n",
    "  * les mots présents dans le test mais absents du test_corpus (les \"inconnus\") seront ici simplement ignorés\n",
    "    \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seiU6grVjnry"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW vectors of train_corpus\n",
      "BOW vectors of test_corpus (unknown words are ignored)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_vocab(documents):\n",
    "    w2id = {}\n",
    "    id2w = []\n",
    "    # TODO\n",
    "    return (w2id, id2w)\n",
    "\n",
    "def bow(doc, w2id, id2w):\n",
    "    # TODO\n",
    "    return bow_vector\n",
    "    \n",
    "# TODO : affichage des vecteurs BOW du corpus train, et du corpus test\n",
    "\n",
    "print(\"BOW vectors of train_corpus\")\n",
    "    \n",
    "print(\"BOW vectors of test_corpus (unknown words are ignored)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison avec les Vectorizer de sklearn\n",
    "\n",
    "On peut utiliser la librairie sklearn pour comparer\n",
    "\n",
    "On donne le code ci-dessous, étudiez-le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fEVt0EJ0jnr0",
    "outputId": "749d8303-107c-43b2-bdcf-44269a9a640d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of X_train <class 'scipy.sparse.csr.csr_matrix'>\n",
      "shape of X_train (3, 13)\n",
      "  (0, 5)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 6)\t1\n",
      "  (1, 6)\t2\n",
      "  (1, 12)\t2\n",
      "  (1, 11)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 7)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 6)\t2\n",
      "  (2, 12)\t2\n",
      "  (2, 11)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 9)\t1\n",
      "[[0 1 1 0 0 1 1 0 1 0 1 0 0]\n",
      " [1 0 0 1 0 0 2 1 0 0 0 1 2]\n",
      " [1 0 0 1 1 0 2 0 0 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ici notre corpus de départ (train_corpus) est déjà tokenisé,\n",
    "# donc on déclare le vectorizer avec un tokenizer et preprocessor qui ne fait rien\n",
    "def dummy(document):\n",
    "    return document\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=dummy, preprocessor=dummy)\n",
    "\n",
    "# au départ, le vectorizer est vide : donc ceci génère une erreur\n",
    "#print(vectorizer.vocabulary_)\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "# fit_transform va créer\n",
    "# - le mapping entre id et mots (le vocabulaire)\n",
    "# - et les vecteurs BOW de chaque document du train_corpus\n",
    "#   sous la forme d'une matrice\n",
    "X_train = vectorizer.fit_transform(train_corpus)\n",
    "\n",
    "# la sortie est une matrice T x |V|\n",
    "# - nb lignes = nb documents dans le train\n",
    "# - nb colonnes = taille du vocabulaire\n",
    "# C'est une matrice creuse, le type utilisé est scipy.sparse\n",
    "print(\"type of X_train\", type(X_train))\n",
    "print(\"shape of X_train\", X_train.shape)\n",
    "print(X_train)\n",
    "\n",
    "# on peut l'afficher de manière plus lisible\n",
    "print(X_train.toarray()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'je': 5, \"n'\": 8, 'aime': 1, 'pas': 10, 'ce': 2, 'livre': 6, 'un': 12, 'très': 11, 'complet': 3, ',': 0, 'magique': 7, 'exceptionnel': 4, 'ni': 9}\n",
      "[',' 'aime' 'ce' 'complet' 'exceptionnel' 'je' 'livre' 'magique' \"n'\" 'ni'\n",
      " 'pas' 'très' 'un']\n"
     ]
    }
   ],
   "source": [
    "# voici la correspondance entre id et mots (notre w2id supra)\n",
    "print(vectorizer.vocabulary_)\n",
    "# et la liste des mots (notre id2w)\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_test (2, 13)\n",
      "{'je': 5, \"n'\": 8, 'aime': 1, 'pas': 10, 'ce': 2, 'livre': 6, 'un': 12, 'très': 11, 'complet': 3, ',': 0, 'magique': 7, 'exceptionnel': 4, 'ni': 9}\n",
      "[',' 'aime' 'ce' 'complet' 'exceptionnel' 'je' 'livre' 'magique' \"n'\" 'ni'\n",
      " 'pas' 'très' 'un']\n"
     ]
    }
   ],
   "source": [
    "# pour les documents de test:\n",
    "# on ignore les nouveaux mots (qui sont \"inconnus\" dans le train)\n",
    "# => on utilise la méthode \"transform\" au lieu de fit_transform\n",
    "\n",
    "X_test = vectorizer.transform(test_corpus)\n",
    "print(\"shape of X_test\", X_test.shape)\n",
    "\n",
    "# On peut voir que la taille du vocabulaire est bien constante,\n",
    "# les mots inconnus dans le train ont simplement été ignorés\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u2w0Y84rjnr8"
   },
   "source": [
    "## Vecteurs de documents, avec poids TF-IDF\n",
    "\n",
    "cf. cours\n",
    "\n",
    "$$\\text{tf-idf}(t, d, D) = \\text{tf}(t, d) \\times \\text{idf}(t, D)$$\n",
    "\n",
    "où \n",
    "* `tf` est le **nombre d'occurrences du terme** t dans le document d\n",
    "  * ou bien des variantes du simple nb d'occurrences \n",
    "* `idf` est la **inverse document frequency** du terme t dans l'ensemble de documents D\n",
    "  * si on note $\\mathrm{df}(t,D)$ = le nb de documents dans D dans lesquels le terme t apparaît\n",
    "  * `sklearn` utilise une définition différente de l'IDF classique: \n",
    "$$  \\mathrm{idf}(t, D) = 1 + \\log \\left( \\frac{|D|}{\\mathrm{df}(t, D)}\\right) $$\n",
    "\n",
    "  * et dans le cas ou `smooth_idf = True` $$\\mathrm{idf}(t, D) = 1+ \\log \\left( \\frac{1+|D|}{1+\\mathrm{df}(t, D)}\\right)   $$\n",
    "  \n",
    "### TODO4: le TfidfVectorizer de sklearn\n",
    "\n",
    "**Utilisez** le TfidfVectorizer de sklearn (au lieu de CountVectorizer supra) pour obtenir les vecteurs BOW avec valeurs TF.IDF\n",
    "\n",
    "Le vocabulaire sous-jacent à un corpus peut grossir très rapidement\n",
    "(cf. loi de Zipf!), ce qui augmente donc directement \n",
    "- la taille de l'espace vectoriel de représentation des documents (= le nombre de \"features\" d'entrée)\n",
    "- et le nombre de paramètres des classifieurs appris sur ces représentations\n",
    "- cela peut amener du surapprentissage (overfitting) et/ou un apprentissage moins performant\n",
    "\n",
    "Plusieurs options permettent de filtrer le vocabulaire pris en compte, en ignorant des termes soit trop peu discriminants, soit trop rares etc...\n",
    "\n",
    "**Trouvez** dans la doc quelles options permettent de limiter la taille des vecteurs BOW résultants, en filtrant selon le nb d'occurrences d'un terme ou le nombre de documents dans lesquels apparaît un terme "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = ...\n",
    "X_train_tfidf = ...\n",
    "X_test_sk = ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifieur via sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = CountVectorizer(tokenizer=dummy, \n",
    "                            preprocessor=dummy)\n",
    "X_train = vectorizer.fit_transform(df['train']['tokens'])\n",
    "X_valid = vectorizer.transform(df['valid']['tokens'])\n",
    "\n",
    "\n",
    "Y_train = df['train']['label']\n",
    "Y_valid = df['valid']['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance de classifieur de type \"régression logistique\"\n",
    "\n",
    "clf_f = LogisticRegression(\n",
    "        #random_state=0, \n",
    "        solver='lbfgs',    # algo d'optimisation (ici minimisation de la perte cross-entropie)\n",
    "        multi_class='ovr', # stratégie \"one versus rest\"\n",
    "        penalty='l2', # hyperparamètre de régularisation\n",
    "        C=1.0, # hyperparamètre de régularisation\n",
    "        n_jobs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- apprentissage sur le train -----------\n",
    "# La méthode fit\n",
    "# (vaut pour tous les types de classifieurs ou régresseurs !)\n",
    "clf_f.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédiction et évaluation\n",
    "\n",
    "Dans le cas d'un classifieur \"mono-label\" (i.e. on demande une et une seule classe par revue), la métrique d'évaluation est simplement la **proportion** de revues bien classées par le système.\n",
    "\n",
    "En anglais on parle d'**accuracy**, en français **précision** ou **exactitude**.\n",
    "\n",
    "En outre une **matrice de confusion** permet de représenter quelles classes sont trop/pas assez prédites. Dans le cas de classification binaire, on parle de:\n",
    "- vrais positifs: items de classe 1 prédits 1\n",
    "- faux positifs: items de classe 0 prédits 1\n",
    "- vrais négatifs: items de classe 0 prédits 0\n",
    "- faux négatifs: items de classe 1 prédits 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------- prediction sur le test---------------\n",
    "Y_valid_pred_f = clf_f.predict(X_valid_f)\n",
    "print(\"SCORE OF LOGISTIC REGRESSION ON VALID: %.3f\" % accuracy_score(Y_valid, Y_valid_pred))\n",
    "    \n",
    "\n",
    "# --------- prediction sur le train--------------\n",
    "Y_train_pred_f = clf_f.predict(X_train_f)\n",
    "print(\"SCORE OF LOGISTIC REGRESSION ON TRAIN: %.3f\" % accuracy_score(Y_train, Y_train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Matrice de confusion normalisée\")\n",
    "    else:\n",
    "        print(\"Matrice de confusion, sans normalisation\")\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Etiquette réelle')\n",
    "    plt.xlabel('Etiquette prédite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(confusion_matrix(Y_valid, Y_valid_pred_f), \n",
    "                      classes=id2label, \n",
    "                      title='Matrice de confusion, sans normalisation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on teste si une pondération TFIDF améliore les résultats\n",
    "clf_t = LogisticRegression(\n",
    "        #random_state=0, \n",
    "        solver='lbfgs',    # algo d'optimisation (ici minimisation de la perte cross-entropie)\n",
    "        multi_class='ovr', # stratégie \"one versus rest\"\n",
    "        penalty='l2', # hyperparamètre de régularisation\n",
    "        C=1.0, # hyperparamètre de régularisation\n",
    "        n_jobs=2\n",
    ")\n",
    "\n",
    "clf_t.fit(X_train_t, Y_train)\n",
    "\n",
    "# --------- prediction sur le test---------------\n",
    "Y_valid_pred_t = clf_t.predict(X_valid_t)\n",
    "print(\"SCORE OF LOGISTIC REGRESSION ON VALID (tfidf): %.3f\" % accuracy_score(Y_valid, Y_valid_pred))\n",
    "    \n",
    "\n",
    "# --------- prediction sur le train--------------\n",
    "Y_train_pred_t = clf_t.predict(X_train_t)\n",
    "print(\"SCORE OF LOGISTIC REGRESSION ON TRAIN (tfidf): %.3f\" % accuracy_score(Y_train, Y_train_pred))\n",
    "\n",
    "#@@ => on peut voir que le tfidf n'a pas un gros impact sur les résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un autre type de classifieur linéaire : les SVM\n",
    "\n",
    "### TODO6: Appliquez l'apprentissage, la prédiction et l'évaluation, en utilisant cette fois un classifieur de type **SVM** (**support vector machine**), avec les hyperparamètres par défaut\n",
    "- cf. https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "- Comparez **avec et sans utilisation de Tfidf** (CountVectorizer versus TfidfVectorizer).\n",
    "\n",
    "**NB** : en apprentissage automatique, on distingue:\n",
    "* les **paramètres**, qui sont des variables dont les valeurs sont fixées grâce au processus d'apprentissage. Celui-ci est en général un problème d'optimisation, résolu\n",
    "  * soit de manière \n",
    "  * soit de manière approchée et itérative, auquel cas, en général, les paramètres sont initialisés (au hasard ou avec des valeurs pré-apprises sur d'autres tâches) et ajustés itérativement\n",
    "* des **hyperparamètres**, qui sont des valeurs fixées en amont du processus d'apprentissage\n",
    "  * chaque algo d'apprentissage va de paire avec un certain nombre d'hyperparamètres à choisir avant l'apprentissage\n",
    "  * par exemple, pour un apprentissage de type régression logistique, on peut choisir d'inclure ou pas un terme régularisateur à la fonction de perte (cf. supra l'option penalty='l2' pour l'instance de LogisticRegression)\n",
    "  * et en amont, les divers pré-traitements constituent des hyperparamètres\n",
    "    * utilisation des formes fléchies, des lemmes ou des stems\n",
    "    * pondération tf.idf ou pas\n",
    "    * minusculisation ou pas\n",
    "    * suppression ou pas des accents\n",
    "    * si tf.idf, utilisation diverse des options max_df, min_df, max_features\n",
    "    * etc...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# TODO \n",
    "# apprentissage SVM\n",
    "# **avec et sans utilisation de Tfidf** (CountVectorizer versus TfidfVectorizer)\n",
    "# prédiction\n",
    "# évaluation sur corpus \"valid\" et sur corpus \"train\" (et affichage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le réglage des hyperparamètres (\"tuning hyperparameters\")\n",
    "\n",
    "Certains algorithmes vont avoir des performances très différentes selon les valeurs des hyperparamètres. Rechercher de bonnes valeurs d'hyperparamètres se dit **régler les hyperparamètres** (en anglais **tuning hyperparameters**).\n",
    "\n",
    "Cela reste très expérimental, et fastidieux en particulier parce qu'on ne peut pas isoler les hyperparamètres les uns des autres: la meilleure valeur trouvée pour A, avec B=b, ne sera pas forcément la meilleure valeur pour A avec B=b'.\n",
    "\n",
    "On en est simplement réduit à tester plusieurs combinaisons d'hyperparamètres, et choisir la meilleure sur ces tests. Plusieurs stratégies existent pour définir les combinaisons à tester:\n",
    "* par tatonnement\n",
    "  * éventuellement ok pour commencer à avoir une idée de l'ordre de grandeur des valeurs fonctionnant bien\n",
    "  * mais il est recommandé d'avoir ensuite une approche plus systématique\n",
    "* avec une **recherche en grille** (**grid search**): on teste de manière systématique les combinaisons de valeurs\n",
    "  * par exemple on teste toutes les combinaisons avec A prenant les valeurs a, a'', a''' et B prenant les valeurs b, b' => 6 combinaisons.\n",
    "  * mais le nb de combinaisons à tester peut devenir rapidement trop important\n",
    "* ou encore avec une recherche aléatoire de combinaisons\n",
    "\n",
    "\n",
    "### La validation croisée (\"cross-validation\")\n",
    "\n",
    "\n",
    "**NB**: l'évaluation des performances pour chaque combinaison d'hyperparamètres doit être faite **sur des exemples non utilisés à l'apprentissage**. \n",
    "Une solution serait d'utiliser le corpus valid, mais cela biaiserait les résultats: la meilleure combinaison d'hyperparamètres obtenue serait celle valable pour le corpus valid. On veut garder celui-ci pour une évaluation finale.\n",
    "\n",
    "C'est pourquoi on utilise plutôt la **validation croisée** (**cross-validation**) pour évaluer la performances de chaque combinaison d'hyperparamètes.\n",
    "\n",
    "### BONUS TODO 7: explorez GridSearchCV et lancer une recherche en grille avec la grille d'hyperparamètres fournies infra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# définition des combinaisons d'hyperparamètres à tester\n",
    "# comme l'existence de certains hyperparamètres dépend de la valeur d'autres hyperparamètre,\n",
    "# ces combinaisons sont définies comme des dictionnaires\n",
    "param_grid = [\n",
    "    {'C': [0.001, 0.1, 1, 10, 100], \n",
    "     'kernel': ['linear']},\n",
    "    {'C': [0.1, 1, 10, 100], \n",
    "     'gamma': ['scale','auto', 0.001], # gamma n'est pertinent que si kernel=rbf\n",
    "     'kernel': ['rbf']},\n",
    "    ]\n",
    "\n",
    "# on utilise un vectorizer trouvé en faisant varier diverses options\n",
    "vectorizer = TfidfVectorizer(tokenizer=dummy, preprocessor=dummy,\n",
    "                                            max_features = 10000,\n",
    "                                            min_df = 2,\n",
    "                                            max_df = 0.7)\n",
    "X_train_f = vectorizer.fit_transform(df['train']['lemmas'])\n",
    "X_valid_f = vectorizer.transform(df['valid']['lemmas'])\n",
    "\n",
    "#TODO BONUS grid search\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
